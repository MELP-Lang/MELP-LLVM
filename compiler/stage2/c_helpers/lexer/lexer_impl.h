#ifndef LEXER_IMPL_H
#define LEXER_IMPL_H

/* MELP Stage 2 - Lexer Interface
 * Generated by: YZ_02 (Lexer Specialist)
 * Date: 15 Ocak 2026
 * Phase: 2.0 - Lexer Implementation (Task 2.2)
 * 
 * This header defines the public API for the lexer module.
 * 
 * Design Principles (AUTONOMOUS):
 * - Single responsibility: ONLY tokenization
 * - Standalone module: Peer to parser (NOT orchestrator)
 * - Simple API: tokenize() + cleanup
 * - No parser logic: Only produces tokens
 */

#include "../common/token.h"

/* ============================================================================
 * MAIN API
 * ============================================================================ */

/* Tokenize source code into array of tokens
 * 
 * Parameters:
 *   source         - Input source code (null-terminated string)
 *   out_token_count - Output parameter for token count (including EOF)
 * 
 * Returns:
 *   Token* - Dynamically allocated array of tokens
 *            Caller MUST call free_tokens() when done
 *            Returns NULL on catastrophic error (malloc failure)
 * 
 * Behavior:
 *   - Scans entire source string
 *   - Produces token array (TOKEN_ERROR for invalid characters)
 *   - Always includes TOKEN_EOF as last token
 *   - Skips whitespace (except newlines)
 *   - Skips comments (-- to end of line)
 * 
 * Error Handling:
 *   - Invalid characters â†’ TOKEN_ERROR with message in lexeme
 *   - Line/column tracking for all tokens
 *   - Multiple errors can exist in token stream
 * 
 * Example:
 *   int count;
 *   Token* tokens = tokenize("function main() as numeric\n  return 42\nend_function", &count);
 *   // Use tokens...
 *   free_tokens(tokens, count);
 */
Token* tokenize(const char* source, int* out_token_count);

/* Free token array and associated memory
 * 
 * Parameters:
 *   tokens - Array of tokens returned by tokenize()
 *   count  - Number of tokens (from tokenize out_token_count)
 * 
 * Behavior:
 *   - Frees str_value for tokens that allocated strings
 *   - Frees token array itself
 *   - Safe to call with NULL tokens
 * 
 * Example:
 *   Token* tokens = tokenize(source, &count);
 *   // ... use tokens ...
 *   free_tokens(tokens, count);
 */
void free_tokens(Token* tokens, int count);

/* ============================================================================
 * UTILITY FUNCTIONS (For internal use and testing)
 * ============================================================================ */

/* Create a token with given properties
 * 
 * Note: lexeme is NOT copied, points to source string
 * For TOKEN_ERROR, lexeme should be error message (will be copied)
 */
Token make_token(TokenType type, const char* lexeme, int lexeme_length, int line, int column);

/* Create an error token with message
 * 
 * Error message will be stored in token (allocated copy)
 */
Token make_error_token(const char* message, int line, int column);

/* Character classification helpers */
int is_digit(char c);           /* '0'-'9' */
int is_alpha(char c);           /* 'a'-'z', 'A'-'Z', '_' */
int is_alphanumeric(char c);    /* is_alpha || is_digit */
int is_whitespace(char c);      /* ' ', '\t', '\r' (NOT '\n'!) */

#endif /* LEXER_IMPL_H */
